## 概述

从[第3章](../ch03_linear)开始，我们深入研究了各种模型，从简单到复杂。虽然这些模型在结构和性能上存在着显著差异，但它们的训练和应用方式有着相似之处：需要提前收集和准备好训练数据，模型需要经过充分的训练和优化才能投入使用。用一个或许略显夸张但生动形象的比喻来说：模型的生产就如同在子宫中孕育婴儿一样。人工智能这种“生命”还比较脆弱，难以与外界深入互动。因此，需要一个相对封闭的环境来促使模型生长。生命体进一步进化的关键在于持续适应新环境，迎接新的挑战。同理，模型的训练也需要迈向一个新的阶段，引导模型走进社会生活，让它在持续交互中学习成长。

本章将讨论**强化学习**（Reinforcement Learning，RL）。强化学习并不是某种新型的模型结构，而是一种全新的模型训练方式。其核心在于如何在不确定的环境中（在训练数据尚未完全收集的情况下）训练模型。为了应对这种不确定性，强化学习采用了一种独特的策略：在模型并未完全准备好的情况下，就开始使用模型来助力自身的训练。这一方法类似于人类在现实生活中的学习方式，比如学习骑自行车，通过不断尝试和练习来提高性能。

强化学习包含很多内容，甚至足以成为一门完整的学科。由于需要处理不确定性的环境，强化学习涉及大量的概率分析和复杂的数学推导过程。详细介绍这一切可能需要一本与厚度很大的专著，因此，本章并不打算穷尽强化学习的方方面面，而只是沿着大语言模型的技术路径展开讨论。具体而言，本章将效仿ChatGPT的做法，探讨如何利用**PPO**（Proximal Policy Optimization）技术来优化模型。ChatGPT所采用的优化技术几乎是强化学习的前沿，因此本章将涵盖该领域的大部分关键概念。

## 代码说明

|代码|说明|
|---|---|
|[intuition_model.ipynb](intuition_model.ipynb)| 将大语言模型与评分模型进行直观的联结 |
|[utils.py](utils.py)| 定义游戏以及相应的可视化工具 |
|[value_learning.ipynb](value_learning.ipynb)| 值函数学习 |
|[policy_learning.ipynb](policy_learning.ipynb)| 策略学习 |
|[a2c.ipynb](a2c.ipynb)| 基准线和A2C模型 |
|[llm_ppo.ipynb](llm_ppo.ipynb)| 使用PPO算法优化大语言模型，使得微调之后的模型评分更高 |
|[llm\_ppo\_correct\_dropout.ipynb](llm_ppo_correct_dropout.ipynb)| 与[llm_ppo.ipynb](llm_ppo.ipynb)的目的一样，在脚本中将着重展示如何在PPO算法中正确使用随机失活 |